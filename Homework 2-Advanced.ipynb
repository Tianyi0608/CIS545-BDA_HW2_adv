{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIS 545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fce4124e1afc58fd5c460dd87cf0ad5d",
     "grade": false,
     "grade_id": "cell-073549de4762fe02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 2, Advanced Part: Iterative Spark Computations\n",
    "### Worth 20 points in total\n",
    "\n",
    "Building upon your experiences with graph data, we will now use Spark to compute PageRank.  Following our discussion of graphs, we will use an edge list, which is a variation of the adjacency list.\n",
    "\n",
    "# Step 5: PageRank\n",
    "\n",
    "Many of you may already know PageRank computation by its reputation:  it is used to measure the importance of a Web page.  (Contrary to popular belief, PageRank is named after Larry Page, not Web pages…)  PageRank is actually a tweaked version of a network centrality measure called *eigenvector centrality*.  One way to implement PageRank is as an iterative computation.  We take each graph node $x$ and in iteration 0 assign it a corresponding PageRank $p_x$:\n",
    "\n",
    "$p_x^0= 1 / N$\n",
    "\n",
    "where $N$ is the total number of nodes.\n",
    "\n",
    "Now in each iteration $i$ we recompute:\n",
    "\n",
    "$p_x^{(i)} = \\alpha * \\Sigma_{j \\in B(x)} (1 / N_j) p_j^{(i-1)} + \\beta$\n",
    "\n",
    "![Graph](graph.png)\n",
    "\n",
    "Where $B(x)$ is the set of nodes linking to node $x$, and $N_j$ is the outdegree of each such node $j$.  Typically, repeating the PageRank computation for a number of iterations (15 or so) results in convergence within an acceptable tolerance.  For this assignment we’ll assume $\\beta = 0.15$ and $\\alpha = 0.85$ (anecdoctally these are the most common values used in practice).\n",
    "\n",
    "*Example*. In the figure to the right, nodes $j_1$ and $j_2$ represent the back-link set $B(x)$ for node $x$.  $N_{j1}$ is 3 and $N_{j2}$ is 2.  Thus in each iteration $i$, we recompute the PageRank score for $x$ by adding half of the PageRank score for $j_2$ and a third of the PageRank score of $j_3$ (both from the previous iteration $i-1$).\n",
    "\n",
    "*Hint*.  Build some “helper” DataFrames.  We suggest at least 2 DataFrames, where the first is used the build the second, and the second is used in your solution:\n",
    "1. a DataFrame with each from_node and the proportion of weight it transfers to each outgoing edge.  For instance, if the from_node is node j then the proportion of weight should be $1/N_j$.\n",
    "2. a DataFrame, again with the from_node, each node it transfers weight to, and the proportion of weight computed in (1).  For instance, if the `from_node` is $j$ and the to_node is $x$, then the tuple should be $(j, x, 1/N_j)$.\n",
    "\n",
    "*Submission*. See the external document for submission information.  Remember to first do the basic part of **Homework 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e4e3641a14a132ef730d779d0acf8bb",
     "grade": false,
     "grade_id": "cell-27e42d519023937f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5.1  Initialization and Marshalling\n",
    "\n",
    "### 5.1.1 Spark setup and data load\n",
    "\n",
    "Initialize PySpark as in the basic Homework 2.  Load `pr_graph.txt` as a text file with a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a1450e20f1b6e381f0041e17d4ae0f1f",
     "grade": false,
     "grade_id": "cell-e10254e4023e8384",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Read pr_graph.txt as SDF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import os\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/ipython3'\n",
    "\n",
    "spark = SparkSession.builder.appName('Graphs-HW2-Adv').getOrCreate()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pr_sdf = spark.read.csv('pr_graph.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  _c0|\n",
      "+-----+\n",
      "|1 2 0|\n",
      "|1 3 0|\n",
      "|1 4 0|\n",
      "|1 5 0|\n",
      "|2 3 0|\n",
      "|2 1 0|\n",
      "|2 5 0|\n",
      "|3 2 0|\n",
      "|3 3 0|\n",
      "|4 5 0|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "78d17ceb04a6bd9bb14616aa2084f303",
     "grade": true,
     "grade_id": "cell-381f3795134180e1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (2 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (2 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7ec107f2fa3cb34bed338e550a77f974",
     "grade": false,
     "grade_id": "cell-552afdf6f48b8192",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 5.1.2 Wrangling the Graph Data\n",
    "\n",
    "As you might have noticed, the `pr_sdf` you loaded in 5.1.1 contains a single column of three values. In this section you will need to use the _SPLIT()_ function to update `pr_sdf` to have 3 columns, and you must cast all three columns to integers: <br>\n",
    "* `from_node` (int)\n",
    "* `to_node` (int)\n",
    "* `reserved` (int)\n",
    "\n",
    "_Hint_: The split function in Spark can be called directly from Spark SQL (`SELECT SPLIT(column,’del’). . .`) or by `import`ing `pyspark.sql.functions` and referring to the function in Python.\n",
    "\n",
    "You may need to cast your columns since they start off as strings.  In Python, you can call `my_sdf.column.cast(‘type’)` to convert data types.  In SQL it’s `SELECT CAST(my_sdf.column AS type).`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d95e8c60ae46a9a549d62c3f8813ea9d",
     "grade": false,
     "grade_id": "cell-71e6535dce3b665b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Convert pr_sdf into (from_node, to_node, reserved) with integer fields\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import pyspark.sql.functions\n",
    "\n",
    "# df.withColumn(\"_tmp\", split($\"columnToSplit\", \"\\\\.\")).select(\n",
    "#   $\"_tmp\".getItem(0).as(\"col1\"),\n",
    "#   $\"_tmp\".getItem(1).as(\"col2\"),\n",
    "#   $\"_tmp\".getItem(2).as(\"col3\")\n",
    "# ).drop(\"_tmp\")\n",
    "\n",
    "pr_sdf = pr_sdf.select(F.split(pr_sdf._c0, ' ')[0].cast('int').alias('from_node'),\n",
    "                       F.split(pr_sdf._c0, ' ')[1].cast('int').alias('to_node'),\n",
    "                       F.split(pr_sdf._c0, ' ')[2].cast('int').alias('reserved'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+\n",
      "|from_node|to_node|reserved|\n",
      "+---------+-------+--------+\n",
      "|        1|      2|       0|\n",
      "|        1|      3|       0|\n",
      "|        1|      4|       0|\n",
      "|        1|      5|       0|\n",
      "|        2|      3|       0|\n",
      "|        2|      1|       0|\n",
      "|        2|      5|       0|\n",
      "|        3|      2|       0|\n",
      "|        3|      3|       0|\n",
      "|        4|      5|       0|\n",
      "+---------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(from_node,IntegerType,true),StructField(to_node,IntegerType,true),StructField(reserved,IntegerType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_sdf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da75e5e62dfe17eebdad47fcee097234",
     "grade": true,
     "grade_id": "cell-a5991dbb368e604c",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (3 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (3 pts)\n",
    "\n",
    "results = pr_sdf.take(20)\n",
    "\n",
    "if 'from_node' not in pr_sdf.columns:\n",
    "    raise KeyError('Unexpected column names')\n",
    "if 'to_node' not in pr_sdf.columns:\n",
    "    raise KeyError('Unexpected column names')\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (3 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ebb9fa4b3b408a03b18a120e15724fd5",
     "grade": false,
     "grade_id": "cell-2ef5eee506b59c87",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 5.2 Basic PageRank\n",
    "\n",
    "Write the function `pagerank(G, num_iter)` which takes a graph DataFrame G corresponding to your graph, and runs for `num_iter` steps.  It should return a DataFrame with columns (`node_id`, `pagerank`).\n",
    "\n",
    "Initialize your PageRank values for each node in the “base case”.  Then, in each iteration, use the helper DataFrames to compute PageRank scores for each node in the next iteration.\n",
    "\n",
    "You will likely find it easier to express some of the computations in SparkSQL.  If you want to use spark.select, you may find it useful to use the Spark F.udf function to create functions that can be called over each row in the DataFrame.  You can create a function that returns a double as follows:\n",
    "\n",
    "```\n",
    "my_fn = F.udf(lambda x: f(x), DoubleType())\n",
    "```\n",
    "\n",
    "Then you can call it like:\n",
    "```\n",
    "\tmy_sdf.select(my_fn(my_arg)).alias(‘col_name’)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "76ee65d2a35bdd493f376322fee0c989",
     "grade": false,
     "grade_id": "cell-f6113e138edee215",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "# TODO: write the function\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "pr_sdf.createOrReplaceTempView('pr')\n",
    "graph1 = spark.sql('''\n",
    "            SELECT from_node, 1/count(to_node) as weight_proportion\n",
    "            FROM pr\n",
    "            GROUP BY from_node\n",
    "            ORDER BY from_node\n",
    "            ''').cache()\n",
    "graph1.createOrReplaceTempView('gr1')\n",
    "graph2 = spark.sql('''\n",
    "            SELECT pr.from_node, to_node, weight_proportion\n",
    "            FROM pr\n",
    "            LEFT JOIN gr1\n",
    "            ON pr.from_node == gr1.from_node\n",
    "            ORDER BY pr.from_node ASC, to_node ASC\n",
    "            ''')\n",
    "#graph2.schema\n",
    "matrix_g = pd.DataFrame({'1':[0,0,0,0,0,0,0],'2':[0,0,0,0,0,0,0],\n",
    "                         '3':[0,0,0,0,0,0,0],'4':[0,0,0,0,0,0,0],\n",
    "                         '5':[0,0,0,0,0,0,0],'6':[0,0,0,0,0,0,0],'7':[0,0,0,0,0,0,0]})\n",
    "gr = graph2.toPandas()\n",
    "f = gr.from_node.values.tolist()\n",
    "t = gr.to_node.values.tolist()\n",
    "w = gr.weight_proportion.tolist()\n",
    "for i in range(0, len(f)):\n",
    "    matrix_g.iloc[t[i]-1,f[i]-1]=w[i]\n",
    "G = np.array(matrix_g.values.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "schema = StructType([\n",
    "    StructField(\"node_id\", StringType(), True), StructField(\"pagerank\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "def pagerank(G, num_iter):\n",
    "    G.createOrReplaceTempView('pr')\n",
    "    graph1 = spark.sql('''\n",
    "            SELECT from_node, 1/count(to_node) as weight_proportion\n",
    "            FROM pr\n",
    "            GROUP BY from_node\n",
    "            ORDER BY from_node\n",
    "            ''').cache()\n",
    "    graph1.createOrReplaceTempView('gr1')\n",
    "    graph2 = spark.sql('''\n",
    "            SELECT pr.from_node, to_node, gr1.weight_proportion\n",
    "            FROM pr\n",
    "            LEFT JOIN gr1\n",
    "            ON pr.from_node == gr1.from_node\n",
    "            ORDER BY pr.from_node ASC, to_node ASC\n",
    "            ''')\n",
    "#graph2.schema\n",
    "    matrix_g = pd.DataFrame(np.zeros((7,7)), columns=['1','2','3','4','5','6','7'],dtype=int)\n",
    "    gr = graph2.toPandas()\n",
    "    f = gr.from_node.values.tolist()\n",
    "    t = gr.to_node.values.tolist()\n",
    "    w = gr.weight_proportion.tolist()\n",
    "    for i in range(0, len(f)):\n",
    "        matrix_g.iloc[t[i]-1,f[i]-1]=w[i]\n",
    "    gr = np.array(matrix_g.values.tolist())\n",
    "    #print(matrix_g)\n",
    "\n",
    "    step=0\n",
    "    pagerank = np.array([1/7,1/7,1/7,1/7,1/7,1/7,1/7])\n",
    "    while True:\n",
    "        #print(G.dtypes, pagerank.dtypes)\n",
    "        pagerank = 0.85*np.dot(gr,pagerank)+0.15\n",
    "        step = step+1\n",
    "        if step == num_iter:\n",
    "            result = pd.DataFrame({'node_id':[1,2,3,4,5,6,7],'pagerank':pagerank})\n",
    "            result = spark.createDataFrame(result)\n",
    "            return result\n",
    "#pagerank(pr_sdf, 5).show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = np.array([[1,1],[1,0]])\n",
    "b = np.array([1,1])\n",
    "c = np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|node_id|           pagerank|\n",
      "+-------+-------------------+\n",
      "|      4|0.36084700241725365|\n",
      "|      1| 0.4495011122882807|\n",
      "|      6|   0.48955231046338|\n",
      "|      3| 0.6585104089419872|\n",
      "|      2| 0.6985616071170864|\n",
      "|      5| 0.8185605449305454|\n",
      "|      7| 0.8622351388414664|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank(pr_sdf, 5).orderBy(\"pagerank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67bc1797f5b6372460bcfa2ace62a412",
     "grade": true,
     "grade_id": "cell-25ac00723b5824f2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (3 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (3 pts)\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (3 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "439bd68237cada03798387d9311cd709",
     "grade": false,
     "grade_id": "cell-dc2f6927e81d2011",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 5.3 Removal of Self-Loops\n",
    "\n",
    "The existing graph has a few self-loops.  Let's see what happens if you remove them.  For this one, take `pr_sdf` and remove all self-edges, creating `pr_no_loops_sdf`.  Run `pagerank(pr_no_loops_sdf, 5)`, sort in decreasing order by pagerank, and put the results in a list `pageranks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a402c6fd2bec9810f34379d9b8b468ab",
     "grade": false,
     "grade_id": "cell-34495389d5fdb0af",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: create pr_no_loops_sdf and feed it into pagerank.  \n",
    "# The final result should be an ordered list of Rows (nodes and pageranks) called pageranks.\n",
    "#pr_sdf.show()\n",
    "from pyspark.sql.functions import desc\n",
    "pr_sdf.createOrReplaceTempView('pr')\n",
    "pr1 = spark.sql('''\n",
    "            SELECT from_node, to_node\n",
    "            FROM pr\n",
    "            WHERE from_node <> to_node\n",
    "            ''').cache()\n",
    "pr1.createOrReplaceTempView('pr1')\n",
    "graph1 = spark.sql('''\n",
    "            SELECT from_node, 1/count(to_node) as weight_proportion\n",
    "            FROM pr1\n",
    "            GROUP BY from_node\n",
    "            ORDER BY from_node\n",
    "            ''').cache()\n",
    "graph1.createOrReplaceTempView('gr1')\n",
    "pr_no_loops_sdf = spark.sql('''\n",
    "            SELECT pr1.from_node, pr1.to_node, weight_proportion\n",
    "            FROM pr1\n",
    "            LEFT JOIN gr1\n",
    "            ON pr1.from_node == gr1.from_node\n",
    "            ORDER BY pr1.from_node ASC, pr1.to_node ASC\n",
    "            ''')\n",
    "#pr_no_loops_sdf.show()\n",
    "\n",
    "pageranks = pagerank(pr_no_loops_sdf, 5).orderBy(desc('pagerank')).select('pagerank').toPandas()\n",
    "pageranks = pageranks['pagerank'].tolist()\n",
    "# .orderBy(desc('pagerank')).show()\n",
    "#pagerank = list(pagerank['pagerank'].sort_values(by=['pagerank'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8792241643754263,\n",
       " 0.7750296824993153,\n",
       " 0.7306856585644919,\n",
       " 0.5711990636189779,\n",
       " 0.5176590027614371,\n",
       " 0.4685776700442682,\n",
       " 0.3953928831360832]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19ea067a7a2f21ea3742950e9fdfb962",
     "grade": true,
     "grade_id": "cell-68982b11d21e2699",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (2 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "\n",
    "if len(pageranks) != 7:\n",
    "    raise ValueError('Should have 7 nodes!')\n",
    "    \n",
    "\n",
    "print('[CIS 545 Test Cases] (2 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70d389142484cd7d793e2b36c927ada1",
     "grade": false,
     "grade_id": "cell-6220091f4288afc5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 6.1 Analysis of Graph Data\n",
    "\n",
    "We will now use everything we have learned thus far to analyze a dataset. We will be using the yelp_review2.csv dataset we used for the basic part of this assignment. The goal of this section is to learn something about the distribution of reviewers. When given a large dataset such as this one, it is very useful to ask yourself questions like these to build effictive priors to use when designing machine learning models. \n",
    "\n",
    "### 6.1.1\n",
    "First, load the `yelp_review2.csv` and `yelp_business.csv` you downloaded in the basic part of this assignment. Make sure they are avaliable as SQL tables as well. This should be the same code you wrote in the basic part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "892addd969647cee45d82d68d3d21dd6",
     "grade": false,
     "grade_id": "cell-f7197b5323786612",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Load yelp_reviews_sdf and yelp_business_sdf and \n",
    "#       yelp_reviews, yelp_business Spark SQL Tables\n",
    "# yelp_reviews_sdf = ...\n",
    "# yelp_business_sdf = ...\n",
    "# yelp_reviews_sdf.createOrReplaceTempView('yelp_reviews')\n",
    "# yelp_business_sdf.createOrReplaceTempView('yelp_business')\n",
    "\n",
    "yelp_reviews_sdf = spark.read.load('yelp_review2.csv', format='csv',header=True)\n",
    "yelp_business_sdf = spark.read.load('yelp_business.csv', format='csv',header=True)\n",
    "\n",
    "yelp_reviews_sdf.createOrReplaceTempView('yelp_reviews')\n",
    "yelp_business_sdf.createOrReplaceTempView('yelp_business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+----------+------------+-----+------------+-------+--------------------+\n",
      "|         business_id|                name|neighborhood|             address|          city|state|postal_code|  latitude|   longitude|stars|review_count|is_open|          categories|\n",
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+----------+------------+-----+------------+-------+--------------------+\n",
      "|FYWN1wneV18bWNgQj...|    Dental by Design|        null|4855 E Warner Rd,...|     Ahwatukee|   AZ|      85044|33.3306902|-111.9785992|  4.0|          22|      1|Dentists;General ...|\n",
      "|He-G7vWjzVUysIKrf...| Stephen Szabo Salon|        null|  3101 Washington Rd|      McMurray|   PA|      15317|40.2916853| -80.1048999|  3.0|          11|      1|Hair Stylists;Hai...|\n",
      "|KQPW8lFf1y5BT2Mxi...|Western Motor Veh...|        null|6025 N 27th Ave, ...|       Phoenix|   AZ|      85017|33.5249025|-112.1153098|  1.5|          18|      1|Departments of Mo...|\n",
      "|8DShNS-LuFqpEWIp0...|    Sports Authority|        null|5000 Arizona Mill...|         Tempe|   AZ|      85282|33.3831468|-111.9647254|  3.0|           9|      0|Sporting Goods;Sh...|\n",
      "|PfOCPjBrlQAnz__NX...|Brick House Taver...|        null|        581 Howe Ave|Cuyahoga Falls|   OH|      44221|41.1195346| -81.4756898|  3.5|         116|      1|American (New);Ni...|\n",
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+----------+------------+-----+------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_business_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+\n",
      "|           review_id|             user_id|         business_id|stars|      date|                text|useful|funny|cool|\n",
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+\n",
      "|vkVSCC7xljjrAI4UG...|bv2nCi5Qv5vroFiqK...|AEx2SYEUJmTxVVB18...|    5|2016-05-28|Super simple plac...|     0|    0|   0|\n",
      "|n6QzIUObkYshz4dz2...|bv2nCi5Qv5vroFiqK...|VR6GpWIda3SfvPC-l...|    5|2016-05-28|Small unassuming ...|     0|    0|   0|\n",
      "|MV3CcKScW05u5LVfF...|bv2nCi5Qv5vroFiqK...|CKC0-MOWMqoeWf6s-...|    5|2016-05-28|Lester's is locat...|     0|    0|   0|\n",
      "|IXvOzsEMYtiJI0CAR...|bv2nCi5Qv5vroFiqK...|ACFtxLv8pGrrxMm6E...|    4|2016-05-28|Love coming here....|     0|    0|   0|\n",
      "|L_9BTb55X0GDtThi6...|bv2nCi5Qv5vroFiqK...|s2I_Ni76bjJNK9yG6...|    4|2016-05-28|Had their chocola...|     0|    0|   0|\n",
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yelp_reviews_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9432ea5e386bcf6d9922b1d7f99babe5",
     "grade": false,
     "grade_id": "cell-08922ec5e6aadffa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "Just like in the basic part, construct a *directed* graph with edges from users to businesses indicating reviews.\n",
    "\n",
    "To do this, you should extract from `yelp_reviews_sdf` the `user_id` as the `from_node`, the `business_id` as the `to_node`, and the `stars` field as the `score`.  Put this into a dataframe called `review_graph_sdf`, and make it available as a table in SQL called `review_graph`.\n",
    "\n",
    "Some of the values may be null; remove these for `user_id` (`from_node`) or `business_id` (`to_node`)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yelp_reviews_sdf.where(col('user_id').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ac49e4737bf599d582c0998e15683e4b",
     "grade": false,
     "grade_id": "cell-868731f28b3e83c3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create yelp_reivews_sdf and review_graph SQL Table\n",
    "\n",
    "# review_graph_sdf = ...\n",
    "# ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from pyspark.sql.functions import *\n",
    "review_graph_sdf = spark.sql('''\n",
    "            SELECT user_id as from_node, business_id as to_node, stars as score\n",
    "            FROM yelp_reviews\n",
    "            WHERE user_id IS NOT NULL AND business_id IS NOT NULL\n",
    "            ''').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|           from_node|             to_node|score|\n",
      "+--------------------+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|AEx2SYEUJmTxVVB18...|    5|\n",
      "|bv2nCi5Qv5vroFiqK...|VR6GpWIda3SfvPC-l...|    5|\n",
      "|bv2nCi5Qv5vroFiqK...|CKC0-MOWMqoeWf6s-...|    5|\n",
      "|bv2nCi5Qv5vroFiqK...|ACFtxLv8pGrrxMm6E...|    4|\n",
      "|bv2nCi5Qv5vroFiqK...|s2I_Ni76bjJNK9yG6...|    4|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_graph_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee751c2adb45abb7ae4eeb083dfe63eb",
     "grade": false,
     "grade_id": "cell-a15a5d063d9d15a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, to make this process a bit faster we will limit our search to a subset of the review_graph.\n",
    "Create a subset of `review_graph_sdf` called `subset_review_graph_sdf` containing only business_id's (`to_node`) from the city of `Vaughan`\n",
    "\n",
    "_Hint_: `yelp_business_sdf` contains location information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a6f10aab60d035cbb47b972e4918d421",
     "grade": false,
     "grade_id": "cell-43641194c6a9ac22",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create subset_review_graph_sdf and SQL table subset_review_graph\n",
    "\n",
    "# subset_review_graph_sdf = ...\n",
    "review_graph_sdf.createOrReplaceTempView('review_graph')\n",
    "subset_review_graph_sdf = spark.sql('''\n",
    "            SELECT *\n",
    "            FROM review_graph\n",
    "            WHERE to_node IN (SELECT business_id FROM yelp_business WHERE city == 'Vaughan')            \n",
    "            ''').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|           from_node|             to_node|score|\n",
      "+--------------------+--------------------+-----+\n",
      "|40OP-bcwLep3I1nHN...|WFB1fn8rWNukmmIfT...|    5|\n",
      "|nOTl4aPC4tKHK35T3...|D-lzSVYyaobiguo7t...|    3|\n",
      "|FEg8v92qx3kK4Hu4T...|AoW2A9bi4g_0AS2uK...|    5|\n",
      "|FEg8v92qx3kK4Hu4T...|iGqGde420TlBrKcU4...|    3|\n",
      "|JGZV0RT2Z7sBDv938...|NqKrfQmxethHjz59P...|    2|\n",
      "|C1pox_TJtah6daXL-...|D0tYz9YSVTP5cZgxp...|    5|\n",
      "|H6uC5xkIvsCUE4cZZ...|5GCaHoHo547U7wkco...|    1|\n",
      "|G-LPOI3oW9T24kEy7...|R1wv7_R7i8aCD1Olu...|    1|\n",
      "|ZESB_yTuNTchP-ReY...|S4QVuwfzn9T2-torV...|    1|\n",
      "|ZESB_yTuNTchP-ReY...|WxfaCysgEvkBxUWo-...|    4|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_review_graph_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f6bc3fb73a60fec4552cb9f501d53f9e",
     "grade": true,
     "grade_id": "cell-af4a91d827456e4f",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (3 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (3 pts)\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (3 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b814d5c4f9f29f5aa9e8d09e692613e4",
     "grade": false,
     "grade_id": "cell-dc921400b332bbaf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 6.1.2 A greedy approach\n",
    "\n",
    "The goal of this section is find out the minimum number of unique reviewers needed to review half of the businesses in Vaughan. \n",
    "\n",
    "First, check that you have the correct number of business below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fc827c29db555d4e535f1f36a4a0a9b5",
     "grade": false,
     "grade_id": "cell-63ccf241231bced9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Select all unique businesses from subset_review_graph and put in them in \n",
    "#       a new sdf called business_sdf keeping the column name to_node\n",
    "\n",
    "# business_sdf = ...\n",
    "subset_review_graph_sdf.createOrReplaceTempView('subset_review_graph')\n",
    "business_sdf= spark.sql('''\n",
    "            SELECT DISTINCT to_node \n",
    "            FROM subset_review_graph\n",
    "            ''').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             to_node|\n",
      "+--------------------+\n",
      "|Q7oKidQDV52LEPfiw...|\n",
      "|gxA-C5tbo0I1xxVyT...|\n",
      "|RMjCnixEY5i12Ciqn...|\n",
      "|JUFIUjadF1rCMQJ01...|\n",
      "|CqTPLUHBM9AM3TEqP...|\n",
      "|Us67uenyjsmaqqgZ6...|\n",
      "|5GAXZ7gJ81TSR0-Q6...|\n",
      "|NalTCdZL6gFsXxaQT...|\n",
      "|iuya9nmV6ievrTNSO...|\n",
      "|HtNjfGlSm5JmmcqaV...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "576535b9f7399e2d01f0e3f0671ca5a0",
     "grade": true,
     "grade_id": "cell-6365dcf917ba139e",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (2 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "\n",
    "if business_sdf.count() != 768:\n",
    "    raise ValueError('You do not have the correct number of businesses')\n",
    "\n",
    "print('[CIS 545 Test Cases] (2 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2f8d8ca6f3496bd391c384160c03858",
     "grade": false,
     "grade_id": "cell-72893ebb4b075741",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, find the least number of people needed to cover at least 50% (384) of the businesses. There are many ways to do this, the most straightfoward is to take a greedy approach. First find the user with most reviewes from subset_review_graph, then remove all the businesses they reviewed and repeat the process. \n",
    "\n",
    "_Note_: This, like most of big data problems, might take a while to run.\n",
    "\n",
    "_Hint_: If you pass the test case below your algorithm is optimal enough :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8a5d0216da00137718e896f4815be389",
     "grade": false,
     "grade_id": "cell-42012684e82f9819",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           from_node|count|\n",
      "+--------------------+-----+\n",
      "|CxDOIDnH8gp9KXzpB...|  160|\n",
      "|A-IkCqnYosZa49XD9...|  132|\n",
      "|Wu0yySWcHQ5tZ_59H...|   79|\n",
      "|6-g1Aw92UoDijvc4k...|   77|\n",
      "|WeVkkF5L39888IPPl...|   69|\n",
      "|2CALR5iCk-ZkyFcKJ...|   59|\n",
      "|qCYSdhsOzHBKT-V72...|   49|\n",
      "|5snWEoA7Qsu-H7nY4...|   40|\n",
      "|F3dKpfp0EpxkL-rDZ...|   38|\n",
      "|l7sZTLRUBK0k3xjRT...|   36|\n",
      "|rrVtOCkC50Bv_hA7j...|   34|\n",
      "|Em4XpeXTKXVy4GjBL...|   31|\n",
      "|0J9mtVJ4_QGsXYVK8...|   30|\n",
      "|ludl6VlDreQcK4ZgO...|   30|\n",
      "|EQecQy0e8i8caL5Zh...|   29|\n",
      "|aR6vaoYj_SuTmz7EZ...|   28|\n",
      "|MZSXGjRozn8mNMUtP...|   27|\n",
      "|SJJWIZLPGXtGTy9UE...|   26|\n",
      "|XChCfeJ6Yx2NDJIpI...|   25|\n",
      "|65yB0ydGXOZ_-T6J_...|   24|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "160\n",
      "256\n",
      "289\n",
      "317\n",
      "341\n",
      "359\n",
      "374\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find the minimum number of users (from_node) needed to cover at least 384\n",
    "#       business. Store all those users' IDs in a new SDF called top_users_sdf\n",
    "#       top_users_sdf should have 1 column called user_id\n",
    "\n",
    "# ... \n",
    "# ...\n",
    "# top_users_sdf = ...\n",
    "schema = StructType([\n",
    "            StructField(\"user_id\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "busi_num=0\n",
    "top_users_sdf = spark.createDataFrame([{'user_id':''}],schema).cache()\n",
    "subset_review_graph_sdf = subset_review_graph_sdf.select('from_node','to_node').cache()\n",
    "subset_review_graph_sdf.groupBy('from_node').count().orderBy(desc('count')).show()\n",
    "while True:\n",
    "    find_top = subset_review_graph_sdf.groupBy('from_node').count().orderBy(desc('count')).take(1)\n",
    "    most_review_user = find_top[0][0]\n",
    "    sdf = spark.createDataFrame([{'user_id':most_review_user}],schema).cache()\n",
    "    top_users_sdf = top_users_sdf.union(sdf).cache()\n",
    "    busi_num = busi_num + find_top[0][1]\n",
    "    print(busi_num)\n",
    "    if busi_num>= 384 or find_top==None:\n",
    "        break\n",
    "    list_sdf = subset_review_graph_sdf.where(subset_review_graph_sdf['from_node']==most_review_user).cache()\n",
    "    subset_review_graph_sdf = subset_review_graph_sdf.join(list_sdf, subset_review_graph_sdf.to_node==list_sdf.to_node, 'leftanti').cache()\n",
    "    #subset_review_graph_sdf.show()\n",
    "top_users_sdf = top_users_sdf.where(col('user_id')!='').cache()\n",
    "# top_users_sdf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             user_id|\n",
      "+--------------------+\n",
      "|CxDOIDnH8gp9KXzpB...|\n",
      "|A-IkCqnYosZa49XD9...|\n",
      "|6-g1Aw92UoDijvc4k...|\n",
      "|WeVkkF5L39888IPPl...|\n",
      "|2CALR5iCk-ZkyFcKJ...|\n",
      "|Wu0yySWcHQ5tZ_59H...|\n",
      "|F3dKpfp0EpxkL-rDZ...|\n",
      "|ludl6VlDreQcK4ZgO...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_users_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26131a70bef0b0a75ad1ac7f070605da",
     "grade": true,
     "grade_id": "cell-ea5d09562ca749c9",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "if top_users_sdf.count() > 9:\n",
    "    raise ValueError('You can cover 384 business with less users')\n",
    "    \n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
